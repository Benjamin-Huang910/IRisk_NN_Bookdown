<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Single-Layer NN Notes | Neural Nets from Scratch</title>
  <meta name="description" content="iRisk Labs SP-2024 Project" />
  <meta name="generator" content="bookdown 0.38 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Single-Layer NN Notes | Neural Nets from Scratch" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="iRisk Labs SP-2024 Project" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Single-Layer NN Notes | Neural Nets from Scratch" />
  
  <meta name="twitter:description" content="iRisk Labs SP-2024 Project" />
  

<meta name="author" content="Daniel Polites" />


<meta name="date" content="2024-04-03" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="digit-model.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Neural Nets from Scratch</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Intro</a>
<ul>
<li class="chapter" data-level="1.0.1" data-path="index.html"><a href="index.html#setup"><i class="fa fa-check"></i><b>1.0.1</b> Setup</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="single-layer-nn-notes.html"><a href="single-layer-nn-notes.html"><i class="fa fa-check"></i><b>2</b> Single-Layer NN Notes</a>
<ul>
<li class="chapter" data-level="2.1" data-path="single-layer-nn-notes.html"><a href="single-layer-nn-notes.html#model-form"><i class="fa fa-check"></i><b>2.1</b> Model Form</a></li>
<li class="chapter" data-level="2.2" data-path="single-layer-nn-notes.html"><a href="single-layer-nn-notes.html#activation-functions"><i class="fa fa-check"></i><b>2.2</b> Activation Functions</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="single-layer-nn-notes.html"><a href="single-layer-nn-notes.html#sigmoid"><i class="fa fa-check"></i><b>2.2.1</b> Sigmoid:</a></li>
<li class="chapter" data-level="2.2.2" data-path="single-layer-nn-notes.html"><a href="single-layer-nn-notes.html#relu"><i class="fa fa-check"></i><b>2.2.2</b> ReLU:</a></li>
<li class="chapter" data-level="2.2.3" data-path="single-layer-nn-notes.html"><a href="single-layer-nn-notes.html#softmax"><i class="fa fa-check"></i><b>2.2.3</b> softmax:</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="single-layer-nn-notes.html"><a href="single-layer-nn-notes.html#loss-functions"><i class="fa fa-check"></i><b>2.3</b> Loss Functions</a></li>
<li class="chapter" data-level="2.4" data-path="single-layer-nn-notes.html"><a href="single-layer-nn-notes.html#parameterization"><i class="fa fa-check"></i><b>2.4</b> Parameterization</a></li>
<li class="chapter" data-level="2.5" data-path="single-layer-nn-notes.html"><a href="single-layer-nn-notes.html#network-fitting"><i class="fa fa-check"></i><b>2.5</b> Network Fitting</a></li>
<li class="chapter" data-level="2.6" data-path="single-layer-nn-notes.html"><a href="single-layer-nn-notes.html#gradient-descent"><i class="fa fa-check"></i><b>2.6</b> Gradient Descent</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="single-layer-nn-notes.html"><a href="single-layer-nn-notes.html#beta-intercept"><i class="fa fa-check"></i><b>2.6.1</b> Beta: Intercept</a></li>
<li class="chapter" data-level="2.6.2" data-path="single-layer-nn-notes.html"><a href="single-layer-nn-notes.html#beta-coefficients"><i class="fa fa-check"></i><b>2.6.2</b> Beta: Coefficients</a></li>
<li class="chapter" data-level="2.6.3" data-path="single-layer-nn-notes.html"><a href="single-layer-nn-notes.html#w-intercepts"><i class="fa fa-check"></i><b>2.6.3</b> W: Intercepts</a></li>
<li class="chapter" data-level="2.6.4" data-path="single-layer-nn-notes.html"><a href="single-layer-nn-notes.html#w-coefficients"><i class="fa fa-check"></i><b>2.6.4</b> W: Coefficients</a></li>
<li class="chapter" data-level="2.6.5" data-path="single-layer-nn-notes.html"><a href="single-layer-nn-notes.html#combining"><i class="fa fa-check"></i><b>2.6.5</b> Combining</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="single-layer-nn-notes.html"><a href="single-layer-nn-notes.html#code-example"><i class="fa fa-check"></i><b>2.7</b> Code Example</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="single-layer-nn-notes.html"><a href="single-layer-nn-notes.html#generate-data"><i class="fa fa-check"></i><b>2.7.1</b> Generate Data</a></li>
<li class="chapter" data-level="2.7.2" data-path="single-layer-nn-notes.html"><a href="single-layer-nn-notes.html#parameter-setup"><i class="fa fa-check"></i><b>2.7.2</b> Parameter Setup</a></li>
<li class="chapter" data-level="2.7.3" data-path="single-layer-nn-notes.html"><a href="single-layer-nn-notes.html#output"><i class="fa fa-check"></i><b>2.7.3</b> Output</a></li>
<li class="chapter" data-level="2.7.4" data-path="single-layer-nn-notes.html"><a href="single-layer-nn-notes.html#gradient-descent-1"><i class="fa fa-check"></i><b>2.7.4</b> Gradient Descent</a></li>
<li class="chapter" data-level="2.7.5" data-path="single-layer-nn-notes.html"><a href="single-layer-nn-notes.html#iterate"><i class="fa fa-check"></i><b>2.7.5</b> Iterate</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="single-layer-nn-notes.html"><a href="single-layer-nn-notes.html#vectorized-calculations"><i class="fa fa-check"></i><b>2.8</b> Vectorized Calculations</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="single-layer-nn-notes.html"><a href="single-layer-nn-notes.html#notation-setup"><i class="fa fa-check"></i><b>2.8.1</b> Notation Setup</a></li>
<li class="chapter" data-level="2.8.2" data-path="single-layer-nn-notes.html"><a href="single-layer-nn-notes.html#gradients"><i class="fa fa-check"></i><b>2.8.2</b> gradients</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="digit-model.html"><a href="digit-model.html"><i class="fa fa-check"></i><b>3</b> Digit Model</a>
<ul>
<li class="chapter" data-level="3.1" data-path="digit-model.html"><a href="digit-model.html#binomial-model"><i class="fa fa-check"></i><b>3.1</b> Binomial Model</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="digit-model.html"><a href="digit-model.html#setup-1"><i class="fa fa-check"></i><b>3.1.1</b> Setup</a></li>
<li class="chapter" data-level="3.1.2" data-path="digit-model.html"><a href="digit-model.html#model"><i class="fa fa-check"></i><b>3.1.2</b> Model</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="digit-model.html"><a href="digit-model.html#multinomial-model"><i class="fa fa-check"></i><b>3.2</b> Multinomial Model</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="digit-model.html"><a href="digit-model.html#setup-2"><i class="fa fa-check"></i><b>3.2.1</b> Setup</a></li>
<li class="chapter" data-level="3.2.2" data-path="digit-model.html"><a href="digit-model.html#model-1"><i class="fa fa-check"></i><b>3.2.2</b> Model</a></li>
<li class="chapter" data-level="3.2.3" data-path="digit-model.html"><a href="digit-model.html#model-heatmaps"><i class="fa fa-check"></i><b>3.2.3</b> model heatmaps</a></li>
<li class="chapter" data-level="3.2.4" data-path="digit-model.html"><a href="digit-model.html#no-outside-cells-model"><i class="fa fa-check"></i><b>3.2.4</b> no outside cells model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="multi-layer-nn-notes.html"><a href="multi-layer-nn-notes.html"><i class="fa fa-check"></i><b>4</b> Multi-Layer NN Notes</a>
<ul>
<li class="chapter" data-level="4.1" data-path="multi-layer-nn-notes.html"><a href="multi-layer-nn-notes.html#notation-setup-1"><i class="fa fa-check"></i><b>4.1</b> Notation Setup</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="multi-layer-nn-notes.html"><a href="multi-layer-nn-notes.html#scalars"><i class="fa fa-check"></i><b>4.1.1</b> Scalars</a></li>
<li class="chapter" data-level="4.1.2" data-path="multi-layer-nn-notes.html"><a href="multi-layer-nn-notes.html#x"><i class="fa fa-check"></i><b>4.1.2</b> X</a></li>
<li class="chapter" data-level="4.1.3" data-path="multi-layer-nn-notes.html"><a href="multi-layer-nn-notes.html#w-1"><i class="fa fa-check"></i><b>4.1.3</b> W</a></li>
<li class="chapter" data-level="4.1.4" data-path="multi-layer-nn-notes.html"><a href="multi-layer-nn-notes.html#b-1"><i class="fa fa-check"></i><b>4.1.4</b> b</a></li>
<li class="chapter" data-level="4.1.5" data-path="multi-layer-nn-notes.html"><a href="multi-layer-nn-notes.html#y"><i class="fa fa-check"></i><b>4.1.5</b> Y</a></li>
<li class="chapter" data-level="4.1.6" data-path="multi-layer-nn-notes.html"><a href="multi-layer-nn-notes.html#z"><i class="fa fa-check"></i><b>4.1.6</b> z</a></li>
<li class="chapter" data-level="4.1.7" data-path="multi-layer-nn-notes.html"><a href="multi-layer-nn-notes.html#a"><i class="fa fa-check"></i><b>4.1.7</b> a</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="multi-layer-nn-notes.html"><a href="multi-layer-nn-notes.html#forward-propagation"><i class="fa fa-check"></i><b>4.2</b> Forward Propagation</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="multi-layer-nn-notes.html"><a href="multi-layer-nn-notes.html#setup-3"><i class="fa fa-check"></i><b>4.2.1</b> Setup</a></li>
<li class="chapter" data-level="4.2.2" data-path="multi-layer-nn-notes.html"><a href="multi-layer-nn-notes.html#algorithm"><i class="fa fa-check"></i><b>4.2.2</b> Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="multi-layer-nn-notes.html"><a href="multi-layer-nn-notes.html#backward-propagation"><i class="fa fa-check"></i><b>4.3</b> Backward Propagation</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="multi-layer-nn-notes.html"><a href="multi-layer-nn-notes.html#delta"><i class="fa fa-check"></i><b>4.3.1</b> Delta</a></li>
<li class="chapter" data-level="4.3.2" data-path="multi-layer-nn-notes.html"><a href="multi-layer-nn-notes.html#gradients-1"><i class="fa fa-check"></i><b>4.3.2</b> Gradients</a></li>
<li class="chapter" data-level="4.3.3" data-path="multi-layer-nn-notes.html"><a href="multi-layer-nn-notes.html#algorithm-1"><i class="fa fa-check"></i><b>4.3.3</b> Algorithm</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="multi-layer-nn-model.html"><a href="multi-layer-nn-model.html"><i class="fa fa-check"></i><b>5</b> Multi-Layer NN Model</a>
<ul>
<li class="chapter" data-level="5.1" data-path="multi-layer-nn-model.html"><a href="multi-layer-nn-model.html#generate-data-1"><i class="fa fa-check"></i><b>5.1</b> Generate Data</a></li>
<li class="chapter" data-level="5.2" data-path="multi-layer-nn-model.html"><a href="multi-layer-nn-model.html#functions"><i class="fa fa-check"></i><b>5.2</b> Functions</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="multi-layer-nn-model.html"><a href="multi-layer-nn-model.html#link-functions"><i class="fa fa-check"></i><b>5.2.1</b> Link Functions</a></li>
<li class="chapter" data-level="5.2.2" data-path="multi-layer-nn-model.html"><a href="multi-layer-nn-model.html#loss-functions-1"><i class="fa fa-check"></i><b>5.2.2</b> Loss Functions</a></li>
<li class="chapter" data-level="5.2.3" data-path="multi-layer-nn-model.html"><a href="multi-layer-nn-model.html#misc-helpers"><i class="fa fa-check"></i><b>5.2.3</b> Misc Helpers</a></li>
<li class="chapter" data-level="5.2.4" data-path="multi-layer-nn-model.html"><a href="multi-layer-nn-model.html#forward-propagation-1"><i class="fa fa-check"></i><b>5.2.4</b> Forward Propagation</a></li>
<li class="chapter" data-level="5.2.5" data-path="multi-layer-nn-model.html"><a href="multi-layer-nn-model.html#gradient-descent-iteration"><i class="fa fa-check"></i><b>5.2.5</b> Gradient Descent Iteration</a></li>
<li class="chapter" data-level="5.2.6" data-path="multi-layer-nn-model.html"><a href="multi-layer-nn-model.html#perform-gradient-descent"><i class="fa fa-check"></i><b>5.2.6</b> Perform Gradient Descent</a></li>
<li class="chapter" data-level="5.2.7" data-path="multi-layer-nn-model.html"><a href="multi-layer-nn-model.html#summary-functions"><i class="fa fa-check"></i><b>5.2.7</b> Summary Functions</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="multi-layer-nn-model.html"><a href="multi-layer-nn-model.html#test-1"><i class="fa fa-check"></i><b>5.3</b> Test</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="multi-layer-nn-model.html"><a href="multi-layer-nn-model.html#other"><i class="fa fa-check"></i><b>5.3.1</b> Other</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="multi-layer-nn-model.html"><a href="multi-layer-nn-model.html#next-steps"><i class="fa fa-check"></i><b>5.4</b> Next Steps</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Neural Nets from Scratch</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="single-layer-nn-notes" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Chapter 2</span> Single-Layer NN Notes<a href="single-layer-nn-notes.html#single-layer-nn-notes" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>These are notes for a single-layer neural network, mostly based off of <em>An Introduction to Statistical Learning</em>.</p>
<p>This chapter starts by outlaying some concepts and notation, then proceeds with an example of a single-layer neural network implemented ‘by-hand’. The notation is quite non-standard and will be refined in later chapters.</p>
<hr />
<p>Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani. <em>An Introduction to Statistical Learning: with Applications in R</em>. New York: Springer, 2013.</p>
<hr />
<div id="model-form" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Model Form<a href="single-layer-nn-notes.html#model-form" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We have an input vector of <em>p</em> variables <span class="math inline">\(X = \{x_1, x_2, \dots, x_p\}\)</span>, and an output scalar <em>Y</em>. We want to build a function <span class="math inline">\(f: \mathbb{R}^p \to \mathbb{R}\)</span> to approximate <em>Y</em>.</p>
<p>For a single layer NN, we have an input layer, hidden layer (with <em>K</em> activations), and output layer. Thus, the model’s form is:</p>
<p><span class="math display">\[
\begin{aligned}
f(X) &amp;= \beta_0 + \sum_{k = 1}^K \left[\beta_k * h_k(X)\right] \\
&amp;= \beta_0 + \sum_{k = 1}^K \left[\beta_k * g\left(w_{k0} + \sum_{j = 1}^p[w_{kj} * X_j]\right)\right]
\end{aligned}
\]</span></p>
<p>we have <em>k</em> indexing our hidden layer neurons, <em>j</em> indexing the weights within each neuron as they relate to each input variable <span class="math inline">\(\{1, 2, \dots, p\}\)</span>. <span class="math inline">\(g(\cdot)\)</span> is our activation function.</p>
<hr />
<p>This model form is built in 2 steps:</p>
<p><span class="math inline">\(h_k(X)\)</span> is known as the activation of the <em>k</em>th neuron of the hidden layer; it is denoted <span class="math inline">\(A_k\)</span>:</p>
<p><span class="math display">\[A_k = h_k(X) = g\left(w_{k0} + \sum_{j = 1}^p[w_{kj} * X_j]\right)\]</span></p>
<p>These get fed into the output layer, so that:</p>
<p><span class="math display">\[f(X) = \beta_0 + \sum_{k = 1}^K (\beta_k * A_k)\]</span></p>
</div>
<div id="activation-functions" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Activation Functions<a href="single-layer-nn-notes.html#activation-functions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="sigmoid" class="section level3 hasAnchor" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Sigmoid:<a href="single-layer-nn-notes.html#sigmoid" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="math display">\[g(z) = \frac{e^z}{1 + e^z} = \frac{1}{1 + e^{-z}}\]</span></p>
</div>
<div id="relu" class="section level3 hasAnchor" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> ReLU:<a href="single-layer-nn-notes.html#relu" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="math display">\[
g(z) = (z)_+ =
\begin{cases}
0 &amp; \text{if } z &lt; 0 \\
z &amp; \text{otherwise}
\end{cases}
\]</span></p>
</div>
<div id="softmax" class="section level3 hasAnchor" number="2.2.3">
<h3><span class="header-section-number">2.2.3</span> softmax:<a href="single-layer-nn-notes.html#softmax" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="math display">\[f_s(X) = P(Y = s | X) = \frac{e^{Z_s}}{\sum_{l = 1}^w e^{Z_l}}\]</span></p>
<p>^^ used for the output layer of a categorical response network.</p>
</div>
</div>
<div id="loss-functions" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> Loss Functions<a href="single-layer-nn-notes.html#loss-functions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For a quantitative response variable, typical to use a squared-error loss function:</p>
<p><span class="math display">\[\sum_{i = 1}^n \left[(y_i - f(x_i))^2\right]\]</span></p>
<p>For a qualitative / categorical response variable, typical to use cross-entropy:</p>
<p><span class="math display">\[-\sum_{i = 1}^n \sum_{m = 1}^w [y_{im} * \ln (f_m(x_i))]\]</span></p>
<p>Where <em>w</em> is the number of output categories. The behavior of this function is such that if the correct category is predicted as 1, the loss is 0. Otherwise, higher certainty for the correct category is rewarded for the correct answer, and lower certainty is punished.</p>
<p>The output matrix <em>Y</em> has been transformed using one-hot encoding in this circumstance, that’s how there are multiple output dimensions (details).</p>
<p>Recall that <span class="math inline">\(y_{im}\)</span> can only be 1 for the correct category; otherwise it is 0. So for each observation, only adding one number here to the total loss.</p>
<p>(3B1B also shows the sum of squared loss for the probability of each category)</p>
</div>
<div id="parameterization" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> Parameterization<a href="single-layer-nn-notes.html#parameterization" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For a single-layer neural network, we have 2 parameter matrices; one for the weights of the hidden layer, and one for the weights of the output layer. These are denoted <strong>W</strong> and <strong>B</strong>, respectively.</p>
<p>In <strong>W</strong>, each row represents an input (with the first row being the ‘1’ input / the neuron’s ‘bias’); each column represents a neuron:</p>
<p><span class="math display">\[
\mathbf W =
\begin{bmatrix}
w_{1, 0} &amp; w_{2, 0} &amp; \cdots &amp; w_{K, 0} \\
w_{1, 1} &amp; w_{2, 1} &amp; \cdots &amp; w_{K, 1} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
w_{1, p} &amp; w_{2, p} &amp; \cdots &amp; w_{K, p}
\end{bmatrix}
\]</span></p>
<p>For <strong>B</strong>, each row is a hidden-layer neuron’s activation (&amp; a bias term).</p>
<p>If the output is quantitative, there is only 1 column for the output:</p>
<p><span class="math display">\[
\mathbf B =
\begin{bmatrix}
\beta_{0} \\
\beta_{1} \\
\vdots \\
\beta_{K}
\end{bmatrix}
\]</span></p>
<p>If the output is qualitative, there is one column per output category:</p>
<p><span class="math display">\[
\mathbf B =
\begin{bmatrix}
\beta_{1, 0} &amp; \beta_{2, 0} &amp; \cdots &amp; \beta_{w, 0} \\
\beta_{1, 1} &amp; \beta_{2, 1} &amp; \cdots &amp; \beta_{w, 1} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\beta_{1, K} &amp; \beta_{2, K} &amp; \cdots &amp; \beta_{w, K}
\end{bmatrix}
\]</span></p>
<p>We can combine <strong>W</strong> and <strong>B</strong> into one parameter vector:</p>
<p><span class="math display">\[
\theta =
\begin{bmatrix}
w_{1, 0} \\
w_{2, 0} \\
\vdots \\
w_{K, p} \\
\beta_{0} \\
\beta_{1} \\
\vdots \\
\beta_{K}
\end{bmatrix}
\]</span></p>
<p>Note that <strong>W</strong> is a <span class="math inline">\((p + 1)\times K\)</span> dimension matrix, and <strong>B</strong> is a <span class="math inline">\((K + 1)\times w\)</span> dimension matrix. So, <span class="math inline">\(\theta\)</span> has <span class="math inline">\((p + 1) * K + (K + 1) * w\)</span> total parameters.</p>
</div>
<div id="network-fitting" class="section level2 hasAnchor" number="2.5">
<h2><span class="header-section-number">2.5</span> Network Fitting<a href="single-layer-nn-notes.html#network-fitting" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Starting with a quantitative output. Our goal is to find:</p>
<p><span class="math display">\[\arg \min_{\theta} \sum_{i = 1}^n \mathcal L (y_i, f(x_i))\]</span></p>
<p>We will use a scaled squared-error loss function:</p>
<p><span class="math display">\[\sum_{i = 1}^n \frac{1}{2} \left[(y_i - f(x_i))^2\right]\]</span></p>
<p>The scaling make for easier derivative-taking down the line. Recall that:</p>
<p><span class="math display">\[f(x_i) = \beta_0 + \sum_{k = 1}^K \left[\beta_k * g\left(w_{k0} + \sum_{j = 1}^p[w_{kj} * x_{ij}]\right)\right]\]</span></p>
<p>So, we are trying to find:</p>
<p><span class="math display">\[\arg \min_{\theta} \sum_{i = 1}^n \frac{1}{2} \left[y_i - \left(\beta_0 + \sum_{k = 1}^K \beta_k * g(w_{k0} + \sum_{j = 1}^p w_{kj} x_{ij})\right)\right]^2\]</span></p>
<p>We will denote the summation (our objective function) <span class="math inline">\(\mathcal{C} (\theta)\)</span>.</p>
<p>This is nearly impossible to calculate by taking the derivative with respect to every variable and solving for a simultaneous 0; however, we can approximate solutions via gradient descent.</p>
</div>
<div id="gradient-descent" class="section level2 hasAnchor" number="2.6">
<h2><span class="header-section-number">2.6</span> Gradient Descent<a href="single-layer-nn-notes.html#gradient-descent" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Our goal is to find <span class="math inline">\(\arg \min_{\theta} \mathcal{C} (\theta)\)</span> with gradient descent:</p>
<ol style="list-style-type: decimal">
<li>Start with a guess <span class="math inline">\(\theta^0\)</span> for all parameters in <span class="math inline">\(\theta\)</span>, and set <span class="math inline">\(t = 0\)</span></li>
<li>Iterate until <span class="math inline">\(\mathcal{C} (\theta)\)</span> fails to decrease:
<ul>
<li><span class="math inline">\(\theta^{t + 1} \leftarrow \theta^t - \rho * \nabla\mathcal{C} (\theta)\)</span></li>
</ul></li>
</ol>
<p><span class="math inline">\(\rho\)</span> is our learning rate: it controls how quickly we respond to the gradient. <span class="math inline">\(\nabla\mathcal{C} (\theta)\)</span> points in the direction of the greatest increase, so we subtract it to move in the direction of the greatest decrease. Our change in parameter values is proportional to both the learning rate and the gradient magnitude.</p>
<p>The last step for us is taking the gradient. In our parameter vector, we have two ‘types’ of parameters: those that came from <strong>W</strong>, and those that came from <strong>B</strong>. These can be split further into those which are intercept terms (—&gt; simpler derivatives) or not.</p>
<p>We will start by manipulating the notation of our objective function to make it easier to work with:</p>
<ul>
<li>let <span class="math inline">\(z_{ik} = w_{k0} + \sum_{j = 1}^p w_{kj} x_{ij}\)</span>
<ul>
<li>so <span class="math inline">\(z_{ik}\)</span> is the <em>i</em>th input of the activation function of the <em>k</em>th hidden-layer neuron</li>
</ul></li>
<li>let <span class="math inline">\(\hat y_i = \beta_0 + \sum_{k = 1}^K \beta_k * g(z_{ik})\)</span>
<ul>
<li>so <span class="math inline">\(\hat y_i\)</span> is our <em>i</em>th prediction</li>
</ul></li>
<li>let <span class="math inline">\(\hat \epsilon_i = \hat y_i - y_i\)</span>
<ul>
<li>so <span class="math inline">\(\hat \epsilon_i\)</span> is our <em>i</em>th residual</li>
<li>note that <span class="math inline">\(\hat \epsilon_i = \left(\beta_0 + \sum_{k = 1}^K \beta_k * g(z_{ik})\right) - y_i\)</span></li>
<li>(against convention here because this is a negative residual; playing fast &amp; loose w/ notation)</li>
</ul></li>
<li>because <span class="math inline">\((a - b)^2 = (b - a)^2\)</span>, we will flip <span class="math inline">\(y\)</span> and <span class="math inline">\(\hat y\)</span> in our objective function</li>
</ul>
<p>So we have:</p>
<p><span class="math display">\[
\begin{aligned}
\mathcal{C} (\theta) &amp;= \sum_{i = 1}^n \frac{1}{2} \left[y_i - \left(\beta_0 + \sum_{k = 1}^K \beta_k * g(w_{k0} + \sum_{j = 1}^p w_{kj} x_{ij})\right)\right]^2 \\ \\
&amp;= \sum_{i = 1}^n \frac{1}{2} \left[\left(\beta_0 + \sum_{k = 1}^K \beta_k * g(w_{k0} + \sum_{j = 1}^p w_{kj} x_{ij})\right) - y_i\right]^2 \\ \\
&amp;= \sum_{i = 1}^n \frac{1}{2} \left[\left(\beta_0 + \sum_{k = 1}^K \beta_k * g(z_{ik})\right) - y_i\right]^2 \\ \\
&amp;= \sum_{i = 1}^n \frac{1}{2} \left[\hat y_i - y_i\right]^2 \\ \\
&amp;= \sum_{i = 1}^n \frac{1}{2} \left[\hat \epsilon_i\right]^2
\end{aligned}
\]</span></p>
<p>Taking our derivatives:</p>
<div id="beta-intercept" class="section level3 hasAnchor" number="2.6.1">
<h3><span class="header-section-number">2.6.1</span> Beta: Intercept<a href="single-layer-nn-notes.html#beta-intercept" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial \mathcal{C}}{\partial \beta_0} &amp;= \frac{\partial}{\partial \beta_0} \sum_{i = 1}^n \frac{1}{2} \left[\left(\beta_0 + \sum_{k = 1}^K \beta_k * g(z_{ik})\right) - y_i\right]^2 \\ \\
&amp;= \sum_{i = 1}^n \left[\left(\beta_0 + \sum_{k = 1}^K \beta_k * g(z_{ik})\right) - y_i\right] \\ \\
&amp;= \sum_{i = 1}^n \hat \epsilon_i
\end{aligned}
\]</span></p>
</div>
<div id="beta-coefficients" class="section level3 hasAnchor" number="2.6.2">
<h3><span class="header-section-number">2.6.2</span> Beta: Coefficients<a href="single-layer-nn-notes.html#beta-coefficients" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial \mathcal{C}}{\partial \beta_k} &amp;= \frac{\partial}{\partial \beta_k} \sum_{i = 1}^n \frac{1}{2} \left[\left(\beta_0 + \sum_{k = 1}^K \beta_k * g(z_{ik})\right) - y_i\right]^2 \\ \\
&amp;= \sum_{i = 1}^n \left[\left(\beta_0 + \sum_{k = 1}^K \beta_k * g(z_{ik})\right) - y_i\right] \frac{\partial}{\partial \beta_k} [\beta_k * g(z_{ik})] \\ \\
&amp;= \sum_{i = 1}^n \left[\left(\beta_0 + \sum_{k = 1}^K \beta_k * g(z_{ik})\right) - y_i\right] g(z_{ik}) \\ \\
&amp;= \sum_{i = 1}^n \hat \epsilon_i \ g(z_{ik})
\end{aligned}
\]</span></p>
</div>
<div id="w-intercepts" class="section level3 hasAnchor" number="2.6.3">
<h3><span class="header-section-number">2.6.3</span> W: Intercepts<a href="single-layer-nn-notes.html#w-intercepts" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial \mathcal{C}}{\partial w_{k0}} &amp;= \frac{\partial}{\partial w_{k0}} \sum_{i = 1}^n \frac{1}{2} \left[\left(\beta_0 + \sum_{k = 1}^K \beta_k * g(z_{ik})\right) - y_i\right]^2 \\ \\
&amp;= \sum_{i = 1}^n \left[\left(\beta_0 + \sum_{k = 1}^K \beta_k * g(z_{ik})\right) - y_i\right] \frac{\partial}{\partial w_{k0}} [\beta_k * g(z_{ik})] \\ \\
&amp;= \sum_{i = 1}^n \left[\left(\beta_0 + \sum_{k = 1}^K \beta_k * g(z_{ik})\right) - y_i\right] \beta_k \ \frac{\partial}{\partial w_{k0}} g(z_{ik}) \\ \\
&amp;= \sum_{i = 1}^n \hat \epsilon_i \ \beta_k \ g&#39;(z_{ik})
\end{aligned}
\]</span></p>
<p>note that <span class="math inline">\(\frac{\partial}{\partial w_{k0}} z_{ik} = \frac{\partial}{\partial w_{k0}} \left[w_{k0} + \sum_{j = 1}^p w_{kj} x_{ij}\right] = 1\)</span></p>
</div>
<div id="w-coefficients" class="section level3 hasAnchor" number="2.6.4">
<h3><span class="header-section-number">2.6.4</span> W: Coefficients<a href="single-layer-nn-notes.html#w-coefficients" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial \mathcal{C}}{\partial w_{kj}} &amp;= \frac{\partial}{\partial w_{kj}} \sum_{i = 1}^n \frac{1}{2} \left[\left(\beta_0 + \sum_{k = 1}^K \beta_k * g(z_{ik})\right) - y_i\right]^2 \\ \\
&amp;= \sum_{i = 1}^n \left[\left(\beta_0 + \sum_{k = 1}^K \beta_k * g(z_{ik})\right) - y_i\right] \frac{\partial}{\partial w_{kj}} [\beta_k * g(z_{ik})] \\ \\
&amp;= \sum_{i = 1}^n \left[\left(\beta_0 + \sum_{k = 1}^K \beta_k * g(z_{ik})\right) - y_i\right] \beta_k \ \frac{\partial}{\partial w_{kj}} g(z_{ik}) \\ \\
&amp;= \sum_{i = 1}^n \left[\left(\beta_0 + \sum_{k = 1}^K \beta_k * g(z_{ik})\right) - y_i\right] \beta_k \ g&#39;(z_{ik}) \ \frac{\partial}{\partial w_{kj}} z_{ik} \\ \\
&amp;= \sum_{i = 1}^n \left[\left(\beta_0 + \sum_{k = 1}^K \beta_k * g(z_{ik})\right) - y_i\right] \beta_k \ g&#39;(z_{ik}) \ x_{ij} \\ \\
&amp;= \sum_{i = 1}^n \hat \epsilon_i \ \beta_k \ g&#39;(z_{ik}) \ x_{ij}
\end{aligned}
\]</span></p>
<p>note that <span class="math inline">\(\frac{\partial}{\partial w_{kj}} z_{ik} = \frac{\partial}{\partial w_{kj}} \left[w_{k0} + \sum_{j = 1}^p w_{kj} x_{ij}\right] = x_{ij}\)</span></p>
</div>
<div id="combining" class="section level3 hasAnchor" number="2.6.5">
<h3><span class="header-section-number">2.6.5</span> Combining<a href="single-layer-nn-notes.html#combining" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Given:</p>
<p><span class="math display">\[
\theta =
\begin{bmatrix}
w_{1, 0} \\
w_{2, 0} \\
\vdots \\
w_{K, p} \\
\beta_{0} \\
\beta_{1} \\
\vdots \\
\beta_{K}
\end{bmatrix}
\]</span></p>
<p>and</p>
<p><span class="math display">\[\mathcal{C} (\theta) = \sum_{i = 1}^n \frac{1}{2} \left[\hat \epsilon_i\right]^2\]</span></p>
<p>We have computed:</p>
<p><span class="math display">\[
\nabla \mathcal{C} (\theta) =
\begin{bmatrix}
\frac{\partial \mathcal{C}}{\partial w_{1, 0}}  \\
\frac{\partial \mathcal{C}}{\partial w_{2, 0}} \\
\vdots \\
\frac{\partial \mathcal{C}}{\partial w_{1, 1}} \\
\vdots \\
\frac{\partial \mathcal{C}}{\partial w_{K, p}} \\
\frac{\partial \mathcal{C}}{\partial \beta_{0}} \\
\frac{\partial \mathcal{C}}{\partial \beta_{1}} \\
\vdots \\
\frac{\partial \mathcal{C}}{\partial \beta_{K}}
\end{bmatrix} = \sum_{i = 1}^n
\begin{bmatrix}
\hat \epsilon_i \ \beta_1 \ g&#39;(z_{i1})  \\
\hat \epsilon_i \ \beta_2 \ g&#39;(z_{i2}) \\
\vdots \\
\hat \epsilon_i \ \beta_1 \ g&#39;(z_{i1}) \ x_{i1} \\
\vdots \\
\hat \epsilon_i \ \beta_K \ g&#39;(z_{ik}) \ x_{ip} \\
\hat \epsilon_i \\
\hat \epsilon_i \ g(z_{i1}) \\
\vdots \\
\hat \epsilon_i \ g(z_{ik})
\end{bmatrix}
\]</span></p>
</div>
</div>
<div id="code-example" class="section level2 hasAnchor" number="2.7">
<h2><span class="header-section-number">2.7</span> Code Example<a href="single-layer-nn-notes.html#code-example" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A simple example of using a small single-layer neural network to act on simulated data:</p>
<div id="generate-data" class="section level3 hasAnchor" number="2.7.1">
<h3><span class="header-section-number">2.7.1</span> Generate Data<a href="single-layer-nn-notes.html#generate-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For now, having 3 inputs and combining them to create y, with a random error term. Would like to tweak the setup eventually.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="single-layer-nn-notes.html#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="do">## create data:</span></span>
<span id="cb13-2"><a href="single-layer-nn-notes.html#cb13-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb13-3"><a href="single-layer-nn-notes.html#cb13-3" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb13-4"><a href="single-layer-nn-notes.html#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="single-layer-nn-notes.html#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize Xs</span></span>
<span id="cb13-6"><a href="single-layer-nn-notes.html#cb13-6" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">X1 =</span> <span class="fu">runif</span>(<span class="at">n =</span> n, <span class="at">min =</span> <span class="sc">-</span><span class="dv">10</span>, <span class="at">max =</span> <span class="dv">10</span>),</span>
<span id="cb13-7"><a href="single-layer-nn-notes.html#cb13-7" aria-hidden="true" tabindex="-1"></a>                <span class="at">X2 =</span> <span class="fu">rnorm</span>(<span class="at">n =</span> n, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">10</span>),</span>
<span id="cb13-8"><a href="single-layer-nn-notes.html#cb13-8" aria-hidden="true" tabindex="-1"></a>                <span class="at">X3 =</span> <span class="fu">rexp</span>(<span class="at">n =</span> n, <span class="at">rate =</span> <span class="dv">1</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb13-9"><a href="single-layer-nn-notes.html#cb13-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.matrix</span>(<span class="at">nrow =</span> n,</span>
<span id="cb13-10"><a href="single-layer-nn-notes.html#cb13-10" aria-hidden="true" tabindex="-1"></a>            <span class="at">ncol =</span> p)</span>
<span id="cb13-11"><a href="single-layer-nn-notes.html#cb13-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-12"><a href="single-layer-nn-notes.html#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="co"># get response</span></span>
<span id="cb13-13"><a href="single-layer-nn-notes.html#cb13-13" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> X[, <span class="dv">1</span>] <span class="sc">+</span> <span class="dv">10</span> <span class="sc">*</span> <span class="fu">sin</span>(X[, <span class="dv">2</span>])<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> <span class="dv">10</span> <span class="sc">*</span> X[, <span class="dv">3</span>] <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="at">n =</span> <span class="dv">1000</span>)</span></code></pre></div>
</div>
<div id="parameter-setup" class="section level3 hasAnchor" number="2.7.2">
<h3><span class="header-section-number">2.7.2</span> Parameter Setup<a href="single-layer-nn-notes.html#parameter-setup" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We will have 2 hidden-layer neurons and a single quantitative output, so <strong>W</strong> will be <span class="math inline">\(4 \times 2\)</span> and <strong>B</strong> will be <span class="math inline">\(3 \times 1\)</span>:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="single-layer-nn-notes.html#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="do">## NN properties</span></span>
<span id="cb14-2"><a href="single-layer-nn-notes.html#cb14-2" aria-hidden="true" tabindex="-1"></a>K <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb14-3"><a href="single-layer-nn-notes.html#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="single-layer-nn-notes.html#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="do">## initialize parameter matrices</span></span>
<span id="cb14-5"><a href="single-layer-nn-notes.html#cb14-5" aria-hidden="true" tabindex="-1"></a>W <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="at">data =</span> <span class="fu">runif</span>(<span class="at">n =</span> (p <span class="sc">+</span> <span class="dv">1</span>) <span class="sc">*</span> K, <span class="at">min =</span> <span class="sc">-</span><span class="dv">1</span>, <span class="at">max =</span> <span class="dv">1</span>),</span>
<span id="cb14-6"><a href="single-layer-nn-notes.html#cb14-6" aria-hidden="true" tabindex="-1"></a>            <span class="at">nrow =</span> (p <span class="sc">+</span> <span class="dv">1</span>),</span>
<span id="cb14-7"><a href="single-layer-nn-notes.html#cb14-7" aria-hidden="true" tabindex="-1"></a>            <span class="at">ncol =</span> K)</span>
<span id="cb14-8"><a href="single-layer-nn-notes.html#cb14-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-9"><a href="single-layer-nn-notes.html#cb14-9" aria-hidden="true" tabindex="-1"></a>B <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="at">data =</span> <span class="fu">runif</span>(<span class="at">n =</span> (K <span class="sc">+</span> <span class="dv">1</span>), <span class="at">min =</span> <span class="sc">-</span><span class="dv">1</span>, <span class="at">max =</span> <span class="dv">1</span>),</span>
<span id="cb14-10"><a href="single-layer-nn-notes.html#cb14-10" aria-hidden="true" tabindex="-1"></a>            <span class="at">nrow =</span> (K <span class="sc">+</span> <span class="dv">1</span>),</span>
<span id="cb14-11"><a href="single-layer-nn-notes.html#cb14-11" aria-hidden="true" tabindex="-1"></a>            <span class="at">ncol =</span> <span class="dv">1</span>)</span>
<span id="cb14-12"><a href="single-layer-nn-notes.html#cb14-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-13"><a href="single-layer-nn-notes.html#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="do">## Specify Link Functions &amp; Derivatives:</span></span>
<span id="cb14-14"><a href="single-layer-nn-notes.html#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="co"># identity</span></span>
<span id="cb14-15"><a href="single-layer-nn-notes.html#cb14-15" aria-hidden="true" tabindex="-1"></a><span class="co"># g &lt;- function(x) {x}</span></span>
<span id="cb14-16"><a href="single-layer-nn-notes.html#cb14-16" aria-hidden="true" tabindex="-1"></a><span class="co"># g_prime &lt;- function(x) {1}</span></span>
<span id="cb14-17"><a href="single-layer-nn-notes.html#cb14-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-18"><a href="single-layer-nn-notes.html#cb14-18" aria-hidden="true" tabindex="-1"></a><span class="co"># sigmoid</span></span>
<span id="cb14-19"><a href="single-layer-nn-notes.html#cb14-19" aria-hidden="true" tabindex="-1"></a>g <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {<span class="dv">1</span> <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(<span class="sc">-</span>x))}</span>
<span id="cb14-20"><a href="single-layer-nn-notes.html#cb14-20" aria-hidden="true" tabindex="-1"></a>g_prime <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {<span class="fu">exp</span>(<span class="sc">-</span>x) <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(<span class="sc">-</span>x))<span class="sc">^</span><span class="dv">2</span>}</span>
<span id="cb14-21"><a href="single-layer-nn-notes.html#cb14-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-22"><a href="single-layer-nn-notes.html#cb14-22" aria-hidden="true" tabindex="-1"></a><span class="co"># ReLU</span></span>
<span id="cb14-23"><a href="single-layer-nn-notes.html#cb14-23" aria-hidden="true" tabindex="-1"></a><span class="co"># g &lt;- function(x) {if (x &lt; 0) {0} else {x}}</span></span>
<span id="cb14-24"><a href="single-layer-nn-notes.html#cb14-24" aria-hidden="true" tabindex="-1"></a><span class="co"># g_prime &lt;- function(x) {if (x &lt; 0) {0} else {1}}</span></span></code></pre></div>
</div>
<div id="output" class="section level3 hasAnchor" number="2.7.3">
<h3><span class="header-section-number">2.7.3</span> Output<a href="single-layer-nn-notes.html#output" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>How the NN will calculate the output:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="single-layer-nn-notes.html#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="do">## create output function</span></span>
<span id="cb15-2"><a href="single-layer-nn-notes.html#cb15-2" aria-hidden="true" tabindex="-1"></a>NN_output <span class="ot">&lt;-</span> <span class="cf">function</span>(X, W, B) {</span>
<span id="cb15-3"><a href="single-layer-nn-notes.html#cb15-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cbind</span>(<span class="dv">1</span>, <span class="fu">g</span>(<span class="fu">cbind</span>(<span class="dv">1</span>, X) <span class="sc">%*%</span> W)) <span class="sc">%*%</span> B</span>
<span id="cb15-4"><a href="single-layer-nn-notes.html#cb15-4" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb15-5"><a href="single-layer-nn-notes.html#cb15-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-6"><a href="single-layer-nn-notes.html#cb15-6" aria-hidden="true" tabindex="-1"></a>example <span class="ot">&lt;-</span> <span class="fu">NN_output</span>(<span class="at">X =</span> X,</span>
<span id="cb15-7"><a href="single-layer-nn-notes.html#cb15-7" aria-hidden="true" tabindex="-1"></a>                     <span class="at">W =</span> W,</span>
<span id="cb15-8"><a href="single-layer-nn-notes.html#cb15-8" aria-hidden="true" tabindex="-1"></a>                     <span class="at">B =</span> B)</span>
<span id="cb15-9"><a href="single-layer-nn-notes.html#cb15-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-10"><a href="single-layer-nn-notes.html#cb15-10" aria-hidden="true" tabindex="-1"></a>example[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>]</span></code></pre></div>
<pre><code>## [1] -0.4570299 -0.8227519 -1.0352693 -0.5013235 -0.7197220</code></pre>
</div>
<div id="gradient-descent-1" class="section level3 hasAnchor" number="2.7.4">
<h3><span class="header-section-number">2.7.4</span> Gradient Descent<a href="single-layer-nn-notes.html#gradient-descent-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>for now, looping through each observation’s gradient then taking the sum — much slower than using matrix/arrays, which will eventually happen:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="single-layer-nn-notes.html#cb17-1" aria-hidden="true" tabindex="-1"></a>GD_iteration <span class="ot">&lt;-</span> <span class="cf">function</span>(X, Y, W, B, <span class="at">rho =</span> <span class="dv">1</span>) {</span>
<span id="cb17-2"><a href="single-layer-nn-notes.html#cb17-2" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb17-3"><a href="single-layer-nn-notes.html#cb17-3" aria-hidden="true" tabindex="-1"></a>  <span class="do">## get errors</span></span>
<span id="cb17-4"><a href="single-layer-nn-notes.html#cb17-4" aria-hidden="true" tabindex="-1"></a>  errors <span class="ot">&lt;-</span> <span class="fu">NN_output</span>(<span class="at">X =</span> X, <span class="at">W =</span> W, <span class="at">B =</span> B) <span class="sc">-</span> Y</span>
<span id="cb17-5"><a href="single-layer-nn-notes.html#cb17-5" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb17-6"><a href="single-layer-nn-notes.html#cb17-6" aria-hidden="true" tabindex="-1"></a>  <span class="do">## get each obs&#39; gradient</span></span>
<span id="cb17-7"><a href="single-layer-nn-notes.html#cb17-7" aria-hidden="true" tabindex="-1"></a>  gradient_array_W <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="at">dim =</span> <span class="fu">c</span>((p <span class="sc">+</span> <span class="dv">1</span>), K, <span class="fu">nrow</span>(X)))</span>
<span id="cb17-8"><a href="single-layer-nn-notes.html#cb17-8" aria-hidden="true" tabindex="-1"></a>  gradient_array_B <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="at">dim =</span> <span class="fu">c</span>((K <span class="sc">+</span> <span class="dv">1</span>), <span class="dv">1</span>, <span class="fu">nrow</span>(X)))</span>
<span id="cb17-9"><a href="single-layer-nn-notes.html#cb17-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb17-10"><a href="single-layer-nn-notes.html#cb17-10" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(X)) {</span>
<span id="cb17-11"><a href="single-layer-nn-notes.html#cb17-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-12"><a href="single-layer-nn-notes.html#cb17-12" aria-hidden="true" tabindex="-1"></a>    <span class="do">## W</span></span>
<span id="cb17-13"><a href="single-layer-nn-notes.html#cb17-13" aria-hidden="true" tabindex="-1"></a>    errors_W <span class="ot">&lt;-</span>  <span class="fu">matrix</span>(errors[i],</span>
<span id="cb17-14"><a href="single-layer-nn-notes.html#cb17-14" aria-hidden="true" tabindex="-1"></a>                        <span class="at">nrow =</span> (p <span class="sc">+</span> <span class="dv">1</span>),</span>
<span id="cb17-15"><a href="single-layer-nn-notes.html#cb17-15" aria-hidden="true" tabindex="-1"></a>                        <span class="at">ncol =</span> K)</span>
<span id="cb17-16"><a href="single-layer-nn-notes.html#cb17-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-17"><a href="single-layer-nn-notes.html#cb17-17" aria-hidden="true" tabindex="-1"></a>    B_W <span class="ot">&lt;-</span> <span class="fu">matrix</span>(B[<span class="sc">-</span><span class="dv">1</span>, ],</span>
<span id="cb17-18"><a href="single-layer-nn-notes.html#cb17-18" aria-hidden="true" tabindex="-1"></a>                  <span class="at">nrow =</span> (p <span class="sc">+</span> <span class="dv">1</span>),</span>
<span id="cb17-19"><a href="single-layer-nn-notes.html#cb17-19" aria-hidden="true" tabindex="-1"></a>                  <span class="at">ncol =</span> K,</span>
<span id="cb17-20"><a href="single-layer-nn-notes.html#cb17-20" aria-hidden="true" tabindex="-1"></a>                  <span class="at">byrow =</span> <span class="cn">TRUE</span>)</span>
<span id="cb17-21"><a href="single-layer-nn-notes.html#cb17-21" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb17-22"><a href="single-layer-nn-notes.html#cb17-22" aria-hidden="true" tabindex="-1"></a>    X_W <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>, X[i, ]),</span>
<span id="cb17-23"><a href="single-layer-nn-notes.html#cb17-23" aria-hidden="true" tabindex="-1"></a>                  <span class="at">nrow =</span> (p <span class="sc">+</span> <span class="dv">1</span>),</span>
<span id="cb17-24"><a href="single-layer-nn-notes.html#cb17-24" aria-hidden="true" tabindex="-1"></a>                  <span class="at">ncol =</span> K,</span>
<span id="cb17-25"><a href="single-layer-nn-notes.html#cb17-25" aria-hidden="true" tabindex="-1"></a>                  <span class="at">byrow =</span> <span class="cn">FALSE</span>)</span>
<span id="cb17-26"><a href="single-layer-nn-notes.html#cb17-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-27"><a href="single-layer-nn-notes.html#cb17-27" aria-hidden="true" tabindex="-1"></a>    g_prime_z_W <span class="ot">&lt;-</span> <span class="fu">apply</span>(<span class="at">X =</span> <span class="fu">c</span>(<span class="dv">1</span>, X[i, ]) <span class="sc">%*%</span> W,</span>
<span id="cb17-28"><a href="single-layer-nn-notes.html#cb17-28" aria-hidden="true" tabindex="-1"></a>                         <span class="at">MARGIN =</span> <span class="dv">2</span>,</span>
<span id="cb17-29"><a href="single-layer-nn-notes.html#cb17-29" aria-hidden="true" tabindex="-1"></a>                         <span class="at">FUN =</span> g_prime) <span class="sc">%&gt;%</span></span>
<span id="cb17-30"><a href="single-layer-nn-notes.html#cb17-30" aria-hidden="true" tabindex="-1"></a>      <span class="fu">matrix</span>(<span class="at">nrow =</span> (p <span class="sc">+</span> <span class="dv">1</span>),</span>
<span id="cb17-31"><a href="single-layer-nn-notes.html#cb17-31" aria-hidden="true" tabindex="-1"></a>             <span class="at">ncol =</span> K,</span>
<span id="cb17-32"><a href="single-layer-nn-notes.html#cb17-32" aria-hidden="true" tabindex="-1"></a>             <span class="at">byrow =</span> <span class="cn">FALSE</span>)</span>
<span id="cb17-33"><a href="single-layer-nn-notes.html#cb17-33" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-34"><a href="single-layer-nn-notes.html#cb17-34" aria-hidden="true" tabindex="-1"></a>    del_W <span class="ot">&lt;-</span> errors_W <span class="sc">*</span> B_W <span class="sc">*</span> g_prime_z_W <span class="sc">*</span> X_W</span>
<span id="cb17-35"><a href="single-layer-nn-notes.html#cb17-35" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-36"><a href="single-layer-nn-notes.html#cb17-36" aria-hidden="true" tabindex="-1"></a>    gradient_array_W[ , , i] <span class="ot">&lt;-</span> del_W</span>
<span id="cb17-37"><a href="single-layer-nn-notes.html#cb17-37" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-38"><a href="single-layer-nn-notes.html#cb17-38" aria-hidden="true" tabindex="-1"></a>    <span class="do">## B</span></span>
<span id="cb17-39"><a href="single-layer-nn-notes.html#cb17-39" aria-hidden="true" tabindex="-1"></a>    errors_B <span class="ot">&lt;-</span> <span class="fu">matrix</span>(errors[i],</span>
<span id="cb17-40"><a href="single-layer-nn-notes.html#cb17-40" aria-hidden="true" tabindex="-1"></a>                       <span class="at">nrow =</span> (K <span class="sc">+</span> <span class="dv">1</span>),</span>
<span id="cb17-41"><a href="single-layer-nn-notes.html#cb17-41" aria-hidden="true" tabindex="-1"></a>                       <span class="at">ncol =</span> <span class="dv">1</span>)</span>
<span id="cb17-42"><a href="single-layer-nn-notes.html#cb17-42" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-43"><a href="single-layer-nn-notes.html#cb17-43" aria-hidden="true" tabindex="-1"></a>    g_z_B <span class="ot">&lt;-</span> <span class="fu">apply</span>(<span class="at">X =</span> <span class="fu">c</span>(<span class="dv">1</span>, X[i, ]) <span class="sc">%*%</span> W,</span>
<span id="cb17-44"><a href="single-layer-nn-notes.html#cb17-44" aria-hidden="true" tabindex="-1"></a>                   <span class="at">MARGIN =</span> <span class="dv">2</span>,</span>
<span id="cb17-45"><a href="single-layer-nn-notes.html#cb17-45" aria-hidden="true" tabindex="-1"></a>                   <span class="at">FUN =</span> g) <span class="sc">%&gt;%</span></span>
<span id="cb17-46"><a href="single-layer-nn-notes.html#cb17-46" aria-hidden="true" tabindex="-1"></a>      <span class="fu">c</span>(<span class="dv">1</span>, .) <span class="sc">%&gt;%</span></span>
<span id="cb17-47"><a href="single-layer-nn-notes.html#cb17-47" aria-hidden="true" tabindex="-1"></a>      <span class="fu">matrix</span>(<span class="at">nrow =</span> (K <span class="sc">+</span> <span class="dv">1</span>),</span>
<span id="cb17-48"><a href="single-layer-nn-notes.html#cb17-48" aria-hidden="true" tabindex="-1"></a>             <span class="at">ncol =</span> <span class="dv">1</span>)</span>
<span id="cb17-49"><a href="single-layer-nn-notes.html#cb17-49" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-50"><a href="single-layer-nn-notes.html#cb17-50" aria-hidden="true" tabindex="-1"></a>    del_B <span class="ot">&lt;-</span> errors_B <span class="sc">*</span> g_z_B</span>
<span id="cb17-51"><a href="single-layer-nn-notes.html#cb17-51" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-52"><a href="single-layer-nn-notes.html#cb17-52" aria-hidden="true" tabindex="-1"></a>    gradient_array_B[ , , i] <span class="ot">&lt;-</span> del_B</span>
<span id="cb17-53"><a href="single-layer-nn-notes.html#cb17-53" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb17-54"><a href="single-layer-nn-notes.html#cb17-54" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb17-55"><a href="single-layer-nn-notes.html#cb17-55" aria-hidden="true" tabindex="-1"></a>  <span class="do">## get gradients</span></span>
<span id="cb17-56"><a href="single-layer-nn-notes.html#cb17-56" aria-hidden="true" tabindex="-1"></a>  del_W_all <span class="ot">&lt;-</span> <span class="fu">apply</span>(<span class="at">X =</span> gradient_array_W,</span>
<span id="cb17-57"><a href="single-layer-nn-notes.html#cb17-57" aria-hidden="true" tabindex="-1"></a>                     <span class="at">MARGIN =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>),</span>
<span id="cb17-58"><a href="single-layer-nn-notes.html#cb17-58" aria-hidden="true" tabindex="-1"></a>                     <span class="at">FUN =</span> mean)</span>
<span id="cb17-59"><a href="single-layer-nn-notes.html#cb17-59" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb17-60"><a href="single-layer-nn-notes.html#cb17-60" aria-hidden="true" tabindex="-1"></a>  del_B_all <span class="ot">&lt;-</span> <span class="fu">apply</span>(<span class="at">X =</span> gradient_array_B,</span>
<span id="cb17-61"><a href="single-layer-nn-notes.html#cb17-61" aria-hidden="true" tabindex="-1"></a>                   <span class="at">MARGIN =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>),</span>
<span id="cb17-62"><a href="single-layer-nn-notes.html#cb17-62" aria-hidden="true" tabindex="-1"></a>                   <span class="at">FUN =</span> mean)</span>
<span id="cb17-63"><a href="single-layer-nn-notes.html#cb17-63" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb17-64"><a href="single-layer-nn-notes.html#cb17-64" aria-hidden="true" tabindex="-1"></a>  <span class="do">## perform iteration</span></span>
<span id="cb17-65"><a href="single-layer-nn-notes.html#cb17-65" aria-hidden="true" tabindex="-1"></a>  W_out <span class="ot">&lt;-</span> W <span class="sc">-</span> rho <span class="sc">*</span> del_W_all</span>
<span id="cb17-66"><a href="single-layer-nn-notes.html#cb17-66" aria-hidden="true" tabindex="-1"></a>  B_out <span class="ot">&lt;-</span> B <span class="sc">-</span> rho <span class="sc">*</span> del_B_all</span>
<span id="cb17-67"><a href="single-layer-nn-notes.html#cb17-67" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb17-68"><a href="single-layer-nn-notes.html#cb17-68" aria-hidden="true" tabindex="-1"></a>  <span class="do">## return</span></span>
<span id="cb17-69"><a href="single-layer-nn-notes.html#cb17-69" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">W =</span> W_out,</span>
<span id="cb17-70"><a href="single-layer-nn-notes.html#cb17-70" aria-hidden="true" tabindex="-1"></a>              <span class="at">B =</span> B_out))</span>
<span id="cb17-71"><a href="single-layer-nn-notes.html#cb17-71" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb17-72"><a href="single-layer-nn-notes.html#cb17-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-73"><a href="single-layer-nn-notes.html#cb17-73" aria-hidden="true" tabindex="-1"></a><span class="do">## test run</span></span>
<span id="cb17-74"><a href="single-layer-nn-notes.html#cb17-74" aria-hidden="true" tabindex="-1"></a>iteration <span class="ot">&lt;-</span> <span class="fu">GD_iteration</span>(<span class="at">X =</span> X,</span>
<span id="cb17-75"><a href="single-layer-nn-notes.html#cb17-75" aria-hidden="true" tabindex="-1"></a>                          <span class="at">Y =</span> Y,</span>
<span id="cb17-76"><a href="single-layer-nn-notes.html#cb17-76" aria-hidden="true" tabindex="-1"></a>                          <span class="at">W =</span> W,</span>
<span id="cb17-77"><a href="single-layer-nn-notes.html#cb17-77" aria-hidden="true" tabindex="-1"></a>                          <span class="at">B =</span> B,</span>
<span id="cb17-78"><a href="single-layer-nn-notes.html#cb17-78" aria-hidden="true" tabindex="-1"></a>                          <span class="at">rho =</span> <span class="dv">1</span> <span class="sc">/</span> <span class="dv">100</span>)</span>
<span id="cb17-79"><a href="single-layer-nn-notes.html#cb17-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-80"><a href="single-layer-nn-notes.html#cb17-80" aria-hidden="true" tabindex="-1"></a><span class="do">## in loss:</span></span>
<span id="cb17-81"><a href="single-layer-nn-notes.html#cb17-81" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>((<span class="fu">NN_output</span>(<span class="at">X =</span> X, <span class="at">W =</span> W, <span class="at">B =</span> B) <span class="sc">-</span> Y)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 369063.7</code></pre>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="single-layer-nn-notes.html#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="do">## out loss:</span></span>
<span id="cb19-2"><a href="single-layer-nn-notes.html#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>((<span class="fu">NN_output</span>(<span class="at">X =</span> X, <span class="at">W =</span> iteration<span class="sc">$</span>W, <span class="at">B =</span> iteration<span class="sc">$</span>B) <span class="sc">-</span> Y)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 362779.6</code></pre>
</div>
<div id="iterate" class="section level3 hasAnchor" number="2.7.5">
<h3><span class="header-section-number">2.7.5</span> Iterate<a href="single-layer-nn-notes.html#iterate" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Employ gradient descent until objective function stops decreasing:</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="single-layer-nn-notes.html#cb21-1" aria-hidden="true" tabindex="-1"></a>threshold <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb21-2"><a href="single-layer-nn-notes.html#cb21-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-3"><a href="single-layer-nn-notes.html#cb21-3" aria-hidden="true" tabindex="-1"></a>done_decreasing <span class="ot">&lt;-</span> <span class="cn">FALSE</span></span>
<span id="cb21-4"><a href="single-layer-nn-notes.html#cb21-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-5"><a href="single-layer-nn-notes.html#cb21-5" aria-hidden="true" tabindex="-1"></a>iteration <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb21-6"><a href="single-layer-nn-notes.html#cb21-6" aria-hidden="true" tabindex="-1"></a>iterations <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb21-7"><a href="single-layer-nn-notes.html#cb21-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-8"><a href="single-layer-nn-notes.html#cb21-8" aria-hidden="true" tabindex="-1"></a>iteration<span class="sc">$</span>W <span class="ot">&lt;-</span> W</span>
<span id="cb21-9"><a href="single-layer-nn-notes.html#cb21-9" aria-hidden="true" tabindex="-1"></a>iteration<span class="sc">$</span>B <span class="ot">&lt;-</span> B</span>
<span id="cb21-10"><a href="single-layer-nn-notes.html#cb21-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-11"><a href="single-layer-nn-notes.html#cb21-11" aria-hidden="true" tabindex="-1"></a>iter <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb21-12"><a href="single-layer-nn-notes.html#cb21-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-13"><a href="single-layer-nn-notes.html#cb21-13" aria-hidden="true" tabindex="-1"></a>initial_objective <span class="ot">&lt;-</span> <span class="fu">sum</span>((<span class="fu">NN_output</span>(<span class="at">X =</span> X, <span class="at">W =</span> iteration<span class="sc">$</span>W, <span class="at">B =</span> iteration<span class="sc">$</span>B) <span class="sc">-</span> Y)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb21-14"><a href="single-layer-nn-notes.html#cb21-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-15"><a href="single-layer-nn-notes.html#cb21-15" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> ((<span class="sc">!</span>done_decreasing) <span class="sc">&amp;</span> (iter <span class="sc">&lt;</span> <span class="dv">301</span>)) {</span>
<span id="cb21-16"><a href="single-layer-nn-notes.html#cb21-16" aria-hidden="true" tabindex="-1"></a>  <span class="do">## get input loss</span></span>
<span id="cb21-17"><a href="single-layer-nn-notes.html#cb21-17" aria-hidden="true" tabindex="-1"></a>  in_objective <span class="ot">&lt;-</span> <span class="fu">sum</span>((<span class="fu">NN_output</span>(<span class="at">X =</span> X, <span class="at">W =</span> iteration<span class="sc">$</span>W, <span class="at">B =</span> iteration<span class="sc">$</span>B) <span class="sc">-</span> Y)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb21-18"><a href="single-layer-nn-notes.html#cb21-18" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb21-19"><a href="single-layer-nn-notes.html#cb21-19" aria-hidden="true" tabindex="-1"></a>  <span class="do">## perform iteration</span></span>
<span id="cb21-20"><a href="single-layer-nn-notes.html#cb21-20" aria-hidden="true" tabindex="-1"></a>  iteration <span class="ot">&lt;-</span> <span class="fu">GD_iteration</span>(<span class="at">X =</span> X,</span>
<span id="cb21-21"><a href="single-layer-nn-notes.html#cb21-21" aria-hidden="true" tabindex="-1"></a>                            <span class="at">Y =</span> Y,</span>
<span id="cb21-22"><a href="single-layer-nn-notes.html#cb21-22" aria-hidden="true" tabindex="-1"></a>                            <span class="at">W =</span> iteration<span class="sc">$</span>W,</span>
<span id="cb21-23"><a href="single-layer-nn-notes.html#cb21-23" aria-hidden="true" tabindex="-1"></a>                            <span class="at">B =</span> iteration<span class="sc">$</span>B,</span>
<span id="cb21-24"><a href="single-layer-nn-notes.html#cb21-24" aria-hidden="true" tabindex="-1"></a>                            <span class="at">rho =</span> <span class="dv">1</span> <span class="sc">/</span> <span class="dv">100</span>)</span>
<span id="cb21-25"><a href="single-layer-nn-notes.html#cb21-25" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb21-26"><a href="single-layer-nn-notes.html#cb21-26" aria-hidden="true" tabindex="-1"></a>  <span class="do">## get output loss</span></span>
<span id="cb21-27"><a href="single-layer-nn-notes.html#cb21-27" aria-hidden="true" tabindex="-1"></a>  out_objective <span class="ot">&lt;-</span> <span class="fu">sum</span>((<span class="fu">NN_output</span>(<span class="at">X =</span> X, <span class="at">W =</span> iteration<span class="sc">$</span>W, <span class="at">B =</span> iteration<span class="sc">$</span>B) <span class="sc">-</span> Y)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb21-28"><a href="single-layer-nn-notes.html#cb21-28" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb21-29"><a href="single-layer-nn-notes.html#cb21-29" aria-hidden="true" tabindex="-1"></a>  <span class="do">## evaluate</span></span>
<span id="cb21-30"><a href="single-layer-nn-notes.html#cb21-30" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (<span class="fu">abs</span>(in_objective <span class="sc">-</span> out_objective) <span class="sc">&lt;</span> threshold) {</span>
<span id="cb21-31"><a href="single-layer-nn-notes.html#cb21-31" aria-hidden="true" tabindex="-1"></a>    done_decreasing <span class="ot">&lt;-</span> <span class="cn">TRUE</span></span>
<span id="cb21-32"><a href="single-layer-nn-notes.html#cb21-32" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb21-33"><a href="single-layer-nn-notes.html#cb21-33" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb21-34"><a href="single-layer-nn-notes.html#cb21-34" aria-hidden="true" tabindex="-1"></a>  <span class="co"># print(iter)</span></span>
<span id="cb21-35"><a href="single-layer-nn-notes.html#cb21-35" aria-hidden="true" tabindex="-1"></a>  <span class="co"># print(out_objective)</span></span>
<span id="cb21-36"><a href="single-layer-nn-notes.html#cb21-36" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb21-37"><a href="single-layer-nn-notes.html#cb21-37" aria-hidden="true" tabindex="-1"></a>  iterations[[iter]] <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="fu">matrix</span>(iteration<span class="sc">$</span>W, <span class="at">nrow =</span> <span class="dv">1</span>),</span>
<span id="cb21-38"><a href="single-layer-nn-notes.html#cb21-38" aria-hidden="true" tabindex="-1"></a>                              <span class="fu">matrix</span>(iteration<span class="sc">$</span>B, <span class="at">nrow =</span> <span class="dv">1</span>))</span>
<span id="cb21-39"><a href="single-layer-nn-notes.html#cb21-39" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb21-40"><a href="single-layer-nn-notes.html#cb21-40" aria-hidden="true" tabindex="-1"></a>  iter <span class="ot">&lt;-</span> iter <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb21-41"><a href="single-layer-nn-notes.html#cb21-41" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb21-42"><a href="single-layer-nn-notes.html#cb21-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-43"><a href="single-layer-nn-notes.html#cb21-43" aria-hidden="true" tabindex="-1"></a>final_objective <span class="ot">&lt;-</span> <span class="fu">sum</span>((<span class="fu">NN_output</span>(<span class="at">X =</span> X, <span class="at">W =</span> iteration<span class="sc">$</span>W, <span class="at">B =</span> iteration<span class="sc">$</span>B) <span class="sc">-</span> Y)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb21-44"><a href="single-layer-nn-notes.html#cb21-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-45"><a href="single-layer-nn-notes.html#cb21-45" aria-hidden="true" tabindex="-1"></a><span class="do">## number of iterations</span></span>
<span id="cb21-46"><a href="single-layer-nn-notes.html#cb21-46" aria-hidden="true" tabindex="-1"></a>iter <span class="ot">&lt;-</span> iter <span class="sc">-</span> <span class="dv">1</span></span>
<span id="cb21-47"><a href="single-layer-nn-notes.html#cb21-47" aria-hidden="true" tabindex="-1"></a>iter</span></code></pre></div>
<pre><code>## [1] 300</code></pre>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="single-layer-nn-notes.html#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="do">## loss improvement ratio</span></span>
<span id="cb23-2"><a href="single-layer-nn-notes.html#cb23-2" aria-hidden="true" tabindex="-1"></a>initial_objective</span></code></pre></div>
<pre><code>## [1] 369063.7</code></pre>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="single-layer-nn-notes.html#cb25-1" aria-hidden="true" tabindex="-1"></a>final_objective</span></code></pre></div>
<pre><code>## [1] 99080.37</code></pre>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="single-layer-nn-notes.html#cb27-1" aria-hidden="true" tabindex="-1"></a>final_objective <span class="sc">/</span> initial_objective</span></code></pre></div>
<pre><code>## [1] 0.2684641</code></pre>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="single-layer-nn-notes.html#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="do">## input W</span></span>
<span id="cb29-2"><a href="single-layer-nn-notes.html#cb29-2" aria-hidden="true" tabindex="-1"></a>W</span></code></pre></div>
<pre><code>##            [,1]       [,2]
## [1,]  0.7875380  0.3591272
## [2,]  0.2717206 -0.8873001
## [3,]  0.7644715  0.1074179
## [4,] -0.7902263 -0.5297394</code></pre>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="single-layer-nn-notes.html#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="do">## output W</span></span>
<span id="cb31-2"><a href="single-layer-nn-notes.html#cb31-2" aria-hidden="true" tabindex="-1"></a>iteration<span class="sc">$</span>W</span></code></pre></div>
<pre><code>##             [,1]       [,2]
## [1,]  1.82875350  0.6259373
## [2,]  5.84398877  0.5638595
## [3,] -0.08899907 -0.1430374
## [4,]  7.95982586  2.1499130</code></pre>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="single-layer-nn-notes.html#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="do">## input B</span></span>
<span id="cb33-2"><a href="single-layer-nn-notes.html#cb33-2" aria-hidden="true" tabindex="-1"></a>B</span></code></pre></div>
<pre><code>##            [,1]
## [1,] -0.5508596
## [2,]  0.1190147
## [3,] -0.4893450</code></pre>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="single-layer-nn-notes.html#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="do">## output B</span></span>
<span id="cb35-2"><a href="single-layer-nn-notes.html#cb35-2" aria-hidden="true" tabindex="-1"></a>iteration<span class="sc">$</span>B</span></code></pre></div>
<pre><code>##          [,1]
## [1,] 8.057293
## [2,] 7.665410
## [3,] 3.855817</code></pre>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="single-layer-nn-notes.html#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="do">## plots</span></span>
<span id="cb37-2"><a href="single-layer-nn-notes.html#cb37-2" aria-hidden="true" tabindex="-1"></a>iterations <span class="ot">&lt;-</span> <span class="fu">do.call</span>(rbind, iterations)</span>
<span id="cb37-3"><a href="single-layer-nn-notes.html#cb37-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-4"><a href="single-layer-nn-notes.html#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfcol =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb37-5"><a href="single-layer-nn-notes.html#cb37-5" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="fl">4.1</span>, <span class="dv">2</span>, <span class="fl">2.1</span>))</span>
<span id="cb37-6"><a href="single-layer-nn-notes.html#cb37-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-7"><a href="single-layer-nn-notes.html#cb37-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">ncol</span>(iterations)) {</span>
<span id="cb37-8"><a href="single-layer-nn-notes.html#cb37-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(<span class="at">x =</span> <span class="dv">1</span><span class="sc">:</span>iter,</span>
<span id="cb37-9"><a href="single-layer-nn-notes.html#cb37-9" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> iterations[ , i],</span>
<span id="cb37-10"><a href="single-layer-nn-notes.html#cb37-10" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="dv">19</span>,</span>
<span id="cb37-11"><a href="single-layer-nn-notes.html#cb37-11" aria-hidden="true" tabindex="-1"></a>       <span class="at">main =</span> <span class="fu">paste</span>(<span class="st">&quot;Var&quot;</span>, i),</span>
<span id="cb37-12"><a href="single-layer-nn-notes.html#cb37-12" aria-hidden="true" tabindex="-1"></a>       <span class="at">ylab =</span> <span class="st">&quot;&quot;</span>,</span>
<span id="cb37-13"><a href="single-layer-nn-notes.html#cb37-13" aria-hidden="true" tabindex="-1"></a>       <span class="at">xlab =</span> <span class="st">&quot;&quot;</span>)</span>
<span id="cb37-14"><a href="single-layer-nn-notes.html#cb37-14" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-6-1.png" width="672" /><img src="_main_files/figure-html/unnamed-chunk-6-2.png" width="672" /></p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="single-layer-nn-notes.html#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="do">## return to default</span></span>
<span id="cb38-2"><a href="single-layer-nn-notes.html#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfcol =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-6-3.png" width="672" /></p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="single-layer-nn-notes.html#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar =</span> <span class="fu">c</span>(<span class="fl">5.1</span>, <span class="fl">4.1</span>, <span class="fl">4.1</span>, <span class="fl">2.1</span>))</span></code></pre></div>
</div>
</div>
<div id="vectorized-calculations" class="section level2 hasAnchor" number="2.8">
<h2><span class="header-section-number">2.8</span> Vectorized Calculations<a href="single-layer-nn-notes.html#vectorized-calculations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A wayward attempt at deriving the matrix notation for vectorized operations that result in a simplified <span class="math inline">\(\nabla \mathcal{C}(\theta)\)</span> by avoiding summations, to be replaced by strategic matrix multiplications.</p>
<p>This attempt was abandoned; there’s more fertile ground in re-defining some notation and pursuing multi-layer networks (later chapters).</p>
<div id="notation-setup" class="section level3 hasAnchor" number="2.8.1">
<h3><span class="header-section-number">2.8.1</span> Notation Setup<a href="single-layer-nn-notes.html#notation-setup" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We have our input matrix <span class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[
X =
\begin{bmatrix}
x_{1, 1} &amp; x_{1, 2} &amp; \cdots &amp; x_{1, p} \\
x_{2, 1} &amp; x_{2, 2} &amp; \cdots &amp; x_{2, p} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
x_{n, 1} &amp; x_{n, 2} &amp; \cdots &amp; x_{n, p} \\
\end{bmatrix}
\]</span></p>
<p>each row represents an obs (1-<em>n</em>)</p>
<p>each col represents a var (1-<em>p</em>)</p>
<hr />
<p>our Weights matrix <span class="math inline">\(W\)</span>:</p>
<p><span class="math display">\[
W =
\begin{bmatrix}
w_{1, 1} &amp; w_{2, 1} &amp; \cdots &amp; w_{K, 1} \\
w_{1, 2} &amp; w_{2, 2} &amp; \cdots &amp; w_{K, 2} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
w_{1, p} &amp; w_{2, p} &amp; \cdots &amp; w_{K, p}
\end{bmatrix}
\]</span></p>
<p>each col represents a neuron (1-<em>K</em>)</p>
<p>each row represents a var (1-<em>p</em>)</p>
<hr />
<p>our output layer weight matrix <span class="math inline">\(B\)</span>:</p>
<p><span class="math display">\[
B =
\begin{bmatrix}
\beta_{1} \\
\vdots \\
\beta_{K}
\end{bmatrix}
\]</span></p>
<p>each row represents a neuron (1-<em>K</em>)</p>
<hr />
<p>our bias matrices <span class="math inline">\(b_1\)</span>, <span class="math inline">\(b_2\)</span>:</p>
<p><span class="math display">\[
b_1 =
\begin{bmatrix}
| &amp; | &amp;  &amp; | \\
w_{1, 0} &amp; w_{2, 0} &amp; \cdots &amp; w_{K, 0} \\
| &amp; | &amp;  &amp; |
\end{bmatrix}
\]</span></p>
<p><span class="math display">\[
b_2 = \begin{bmatrix} | \\ \beta_0 \\ | \end{bmatrix}
\]</span></p>
<p>for <span class="math inline">\(b_1\)</span>, each col has a height of <span class="math inline">\(n\)</span> and represents a neuron (1-<em>K</em>)</p>
<p>for <span class="math inline">\(b_2\)</span>, the col has a height of <span class="math inline">\(K\)</span></p>
<hr />
<p>our target layer matrix <span class="math inline">\(Y\)</span>:</p>
<p><span class="math display">\[
Y =
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{bmatrix}
\]</span></p>
<hr />
<p>also, we have defined: <span class="math inline">\(z_{ik} = w_{k0} + \sum_{j = 1}^p w_{kj} x_{ij}\)</span> to get the activation function’s input for a given neuron. We can take the neurons in their totality to define <span class="math inline">\(Z\)</span>:</p>
<p><span class="math display">\[Z = X \cdot W + b_1\]</span></p>
<p>each row represents an obs (1-<em>n</em>)</p>
<p>each col represents a neuron (1-<em>K</em>)</p>
<hr />
<p>our model output is <span class="math inline">\(\hat Y\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
\hat Y = f(X) &amp;= g(Z) \cdot B + b_2 \\
&amp;= g(X \cdot W + b_1) \cdot B + b_2 \\ \\
&amp;= g\left(\begin{bmatrix}
x_{1, 1} &amp; x_{1, 2} &amp; \cdots &amp; x_{1, p} \\
x_{2, 1} &amp; x_{2, 2} &amp; \cdots &amp; x_{2, p} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
x_{n, 1} &amp; x_{n, 2} &amp; \cdots &amp; x_{n, p} \\
\end{bmatrix} \cdot \begin{bmatrix}
w_{1, 1} &amp; w_{2, 1} &amp; \cdots &amp; w_{K, 1} \\
w_{1, 2} &amp; w_{2, 2} &amp; \cdots &amp; w_{K, 2} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
w_{1, p} &amp; w_{2, p} &amp; \cdots &amp; w_{K, p}
\end{bmatrix} + \begin{bmatrix}
| &amp; | &amp;  &amp; | \\
w_{1, 0} &amp; w_{2, 0} &amp; \cdots &amp; w_{K, 0} \\
| &amp; | &amp;  &amp; |
\end{bmatrix}\right) \cdot \begin{bmatrix}
\beta_{1} \\
\vdots \\
\beta_{K}
\end{bmatrix} + \begin{bmatrix} | \\ \beta_0 \\ | \end{bmatrix} \\ \\
&amp;= \begin{bmatrix}
\hat y_1 \\
\hat y_2 \\
\vdots \\
\hat y_n
\end{bmatrix}
\end{aligned}
\]</span></p>
<hr />
<p>our error matrix:</p>
<p><span class="math display">\[\mathbf{\hat \epsilon}= Y - \hat Y\]</span></p>
</div>
<div id="gradients" class="section level3 hasAnchor" number="2.8.2">
<h3><span class="header-section-number">2.8.2</span> gradients<a href="single-layer-nn-notes.html#gradients" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We can now vectorize our gradient, <span class="math inline">\(\nabla \mathcal{C}(\theta)\)</span>:</p>
<div id="b_2" class="section level4 hasAnchor" number="2.8.2.1">
<h4><span class="header-section-number">2.8.2.1</span> b_2<a href="single-layer-nn-notes.html#b_2" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><span class="math display">\[
\begin{aligned}
\nabla \mathcal{C}(b_2) &amp;= \sum_{i = 1}^n \hat \epsilon_i\\
&amp;= [\mathbf 1]^T \mathbf{\hat \epsilon}
\end{aligned}
\]</span></p>
</div>
<div id="b" class="section level4 hasAnchor" number="2.8.2.2">
<h4><span class="header-section-number">2.8.2.2</span> B<a href="single-layer-nn-notes.html#b" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><span class="math display">\[
\begin{aligned}
\nabla \mathcal{C}(B) &amp;=  \sum_{i = 1}^n
\begin{bmatrix}
\hat \epsilon_i\ g(z_{i1}) \\
\hat \epsilon_i\ g(z_{i2}) \\
\vdots \\
\hat \epsilon_i\ g(z_{ik})
\end{bmatrix} \\ \\
&amp;= [g(Z)]^T \cdot \mathbf{\hat \epsilon}
\end{aligned}
\]</span></p>
</div>
<div id="b_1" class="section level4 hasAnchor" number="2.8.2.3">
<h4><span class="header-section-number">2.8.2.3</span> b_1<a href="single-layer-nn-notes.html#b_1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><span class="math display">\[
\begin{aligned}
\nabla \mathcal{C}(b_1) &amp;= \sum_{i = 1}^n
\begin{bmatrix}
\hat \epsilon_i\ \beta_1 \ g&#39;(z_{i1})  \\
\hat \epsilon_i\ \beta_2 \ g&#39;(z_{i2}) \\
\vdots \\
\hat \epsilon_i\ \beta_K \ g&#39;(z_{iK})
\end{bmatrix} \\ \\
&amp;= \left([g&#39;(Z)]^T \cdot \mathbf{\hat \epsilon}\right) \odot B
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\odot\)</span> is the element-wise multiplication operator</p>
</div>
<div id="w" class="section level4 hasAnchor" number="2.8.2.4">
<h4><span class="header-section-number">2.8.2.4</span> W<a href="single-layer-nn-notes.html#w" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><span class="math display">\[
\begin{aligned}
\nabla \mathcal{C}(W) &amp;= \sum_{i = 1}^n
\begin{bmatrix}
\hat \epsilon_i\ \beta_1 \ g&#39;(z_{i1}) \ x_{i1}  \\
\hat \epsilon_i\ \beta_2 \ g&#39;(z_{i2}) \ x_{i2} \\
\vdots \\
\hat \epsilon_i\ \beta_K \ g&#39;(z_{iK}) \ x_{ip}
\end{bmatrix} \\ \\
&amp;= \ ???
\end{aligned}
\]</span></p>

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="digit-model.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/02_Single_Layer_NN_notes.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
